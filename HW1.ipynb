{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understand Decision Tree Learning\n",
    "\n",
    "\n",
    "Attribute | Val | Play outside = yes | Play outside = no  \n",
    "Sunny yes 20 1  \n",
    "Sunny no 15 14  \n",
    "Snow no 10 5  \n",
    "Snow yes 25 10\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ID3 / Info. Gain\n",
    "Pick the root node, the one we gain the most info from\n",
    "The goal is to have the resulting decision tree as small as\n",
    "possible (Occam’s Razor)\n",
    "\n",
    "\n",
    "S is set of examples  \n",
    "P+ is proportion of positive examples in S  \n",
    "P_ is proportion of negative examples in S  \n",
    "\n",
    "\n",
    "if all examples are P+, entropy is 0  \n",
    "if half are P+ and half are P_ entropy is 1  \n",
    "\n",
    "<img src=\"entropy.png\" />\n",
    "\n",
    "<img src=\"info_gain.png\" />\n",
    "#### Where:\n",
    "Sv is the subset of S for which attribute a has value v, and  \n",
    "the entropy of partitioning the data is calculated by weighing the  \n",
    "entropy of each partition by its size relative to the original set\n",
    "\n",
    "#### Example from Above\n",
    "Attribute | Val | Play outside = yes | Play outside = no  \n",
    "Sunny yes 20 1  \n",
    "Sunny no 15 14  \n",
    "Snow no 10 5  \n",
    "Snow yes 25 10\n",
    "\n",
    "What is the Entropy?  \n",
    "What is the gain for each?  \n",
    "##### max(gain) == root node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "Entropy p1:  0.7204024419616616\n",
      "entropy: \n",
      "0.8112781244591328\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.31120000000000003"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\n",
    "# My implementation of entropy\n",
    "#\n",
    "\n",
    "import math\n",
    "\n",
    "# example_set is a list of probabilities\n",
    "def ent(example_set):\n",
    "    total = 0\n",
    "    for prob in example_set:\n",
    "        try:\n",
    "            total += prob * math.log(prob,2)\n",
    "        except ValueError:\n",
    "            continue\n",
    "    return -1. * total\n",
    "print(ent([.5,.5]))\n",
    "\n",
    "### Example from Udacity Intro. to ML \n",
    "\n",
    "print(\"Entropy p1: \", -35/50*math.log(35/50,2) - 35/50*math.log(35/50,2))\n",
    "\n",
    "\n",
    "print(\"entropy: \")\n",
    "print(-.25*math.log(.25,2) - .75*math.log(.75,2))\n",
    "\n",
    "1 - (3/4) * .9184"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example from Udacity Intro. to ML \n",
    "<img src=\"entropy_info_gain.png\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6108643020548935"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\n",
    "# Scipy's implementation of entropy\n",
    "#\n",
    "# scipy uses log base e, instead of base2, \n",
    "# both are fine, just need to pick one and be consistent\n",
    "#\n",
    "\n",
    "import scipy.stats as stat\n",
    "stat.entropy([.7,.3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Dataset (counts) for Part 1   \n",
    "<img src=\"dataset1.png\" />  \n",
    "\n",
    "What is the entropy of the dataset?  \n",
    "What is the Information Gain for each?  \n",
    "What is the root node?  \n",
    "* Going to split on Sunny = yes/no OR Snowy = yes/no\n",
    "\n",
    "\n",
    "### The best split should make the samples \"pure\" ie. homogeneous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PlayPct: 0.700000\n",
      "NoPlayPct: 0.300000\n",
      "Entropy: 0.6109\n"
     ]
    }
   ],
   "source": [
    "# 50 Instances\n",
    "TOTAL = 50.\n",
    "\n",
    "PLAY = 35\n",
    "NO_PLAY = 15\n",
    "\n",
    "PLAY_PCT = 1. * (PLAY / TOTAL)\n",
    "NO_PLAY_PCT = 1. * (NO_PLAY / TOTAL)\n",
    "\n",
    "ent = stat.entropy([PLAY_PCT, NO_PLAY_PCT])\n",
    "\n",
    "# You can see that there are 50 instances in the data set, 35 of which\n",
    "# are positive (Play outside = yes) and the remaining 15 are negative (Play\n",
    "# outside= no)\n",
    "\n",
    "print(\"PlayPct: %f\" % PLAY_PCT)\n",
    "print(\"NoPlayPct: %f\" % NO_PLAY_PCT)\n",
    "print(\"Entropy: %.4f\" % ent)\n",
    "\n",
    "# Sunny/not sunny\n",
    "sunny = {\n",
    "\"sunYesPlayYes\" : 20,\n",
    "\"sunYesPlayNo\" : 1,\n",
    "\"sunNoPlayYes\" : 15,\n",
    "\"sunNoPlayNo\" : 14,\n",
    "}\n",
    "\n",
    "# Snow/not snow\n",
    "snowy = {\n",
    "\"snowNoPlayYes\" : 10,\n",
    "\"snowNoPlayNo\" : 5,\n",
    "\"snowYesPlayYes\" : 25,\n",
    "\"snowYesPlayNo\" : 10,\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Root node should be Sunny = yes/no\n",
    "##### provides the largest reduction in entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Quetion 1b\n",
    "\n",
    "# put into recursive formula what I was doing on paper\n",
    "\n",
    "def induceDTree(data):\n",
    "    # if all T or all F, set classAttribute\n",
    "    if len(set(data)) == 1:\n",
    "        return \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Part II\n",
    "\n",
    "from sklearn import tree\n",
    "X = [[0, 0], [1, 1]]\n",
    "y = [0, 1]\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "clf = clf.fit(X,y)\n",
    "clf.predict([[2.,2.]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Part 2.2. Decision Trees as Features\n",
    "# sklearn, use multiple versions of DTree algorithm\n",
    "# then use those DTs as features for SGD - Stochastic Gradient Descent\n",
    "\n",
    "# turnin -c cis519 -p hw1 hw1.pdf test_labels.txt README hw1.py\n",
    "\n",
    "# BADGES.zip\n",
    "# \n",
    "# badges.modified.data.train --all examples\n",
    "# \n",
    "# badges.modified.data.fold1, fold2, fold3 etc.\n",
    "# \n",
    "# badges.modified.data.test, -task is to label these test_labels.txt\n",
    "# using best performing model\n",
    "# submit in same format as training file, each line should start with label\n",
    "# followed by a space and followed by the actual name\n",
    "\n",
    "# Main goal, learn function label names with +,- correctly\n",
    "#   Use DT algorithm to generate features for learning\n",
    "#   a linear separator.\n",
    "\n",
    "# INPUT: two lower cased strings - first and last names\n",
    "# OUTPUT: + or -\n",
    "\n",
    "# a) FEATURE EXTRACTION AND INSTANCE GENERATION\n",
    "#     what is INSTANCE GENERATION?\n",
    "#     need to extract features frome data\n",
    "#       - extract ten features for each example\n",
    "#          - boolean feature types\n",
    "\n",
    "# For example, consider the name “naoki abe” from the data set. Suppose you\n",
    "# want to extract features corresponding to the first letter “n” in the first name,\n",
    "# you will have 26 Boolean features, one for each letter in the alphabet. Only\n",
    "# the one corresponding to n will be 1 and the rest will be 0. This will give us\n",
    "# 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n",
    "# where the 14th element corresponds to the feature String=First,Position=1,Character=n. \n",
    "# Note that the features defined earlier are actually feature types. In fact, you will have\n",
    "# 260 features of the form String=i,Position=j,Character=k, where i can be\n",
    "# FirstName or LastName, j can be 1, . . . , 5 and k can be a,b,c, ..., z.\n",
    "\n",
    "# In addition to the feature types described above, feel free to include additional features\n",
    "\n",
    "# b) Decision Trees and SGD learning algorithms\n",
    "#     i. use SGD as a baseline  (sklearn.linear model.SGDClassifier)\n",
    "#         -use five fold cross validation to tune parameters (learning rate, error thresh.)\n",
    "#          of the SGD algorithm\n",
    "#            - how do i do five fold CV in sklearn?\n",
    "#    ii. Grow decision tree: use DT package to train a decision tree with the CART\n",
    "#         algorithm available in sklearn\n",
    "#   iii. grow DTs of depth=4, repeat as above limiting depth to 4\n",
    "#          -Decision trees with limited depth are also called decision stumps.\n",
    "\n",
    "#    iv. grow DTs of depth= 8, repeat as above limiting depth to 8\n",
    "#     v. Decision stumps (limited depth DTs) as features, use feature set defined in a)\n",
    "\n",
    "\n",
    "\n",
    "# 2.3 Evaluation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nameToVec(full_name):\n",
    "    feature_vector = []\n",
    "    # nick fausti\n",
    "    # [0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,\n",
    "    #  0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,\n",
    "    #  0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,\n",
    "    #  0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,\n",
    "    #  0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,\n",
    "    #  0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,\n",
    "    #  1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,\n",
    "    #  0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,\n",
    "    #  0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,\n",
    "    #  0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]\n",
    "    # 26 * 10 = 260\n",
    "    # if either name doesn't have at least 5 chars, pad with 0 until len > 129\n",
    "    name_list = full_name.split(\" \")\n",
    "    first = name_list[0]\n",
    "#     print(first)\n",
    "    last = name_list[1]\n",
    "#     print(last)\n",
    "    \n",
    "    place = 0\n",
    "    for letter in first:\n",
    "        if place < 5:\n",
    "            for i in range(26):\n",
    "                feature_vector.append(0)\n",
    "            position = ord(letter) - 97 + (place * 26)\n",
    "#             print(letter + \":\" + str(position))\n",
    "            feature_vector[position] = 1\n",
    "            place += 1\n",
    "    \n",
    "    # pad 0s for first names less than 5 chars\n",
    "    while len(feature_vector) < 130:\n",
    "        feature_vector.append(0)\n",
    "    \n",
    "    place = 5\n",
    "    for letter in last:\n",
    "        if place < 10:\n",
    "            for i in range(26):\n",
    "                feature_vector.append(0)\n",
    "            position = ord(letter) - 97 + (place * 26)\n",
    "#             print(letter + \":\" + str(position))\n",
    "            feature_vector[position] = 1\n",
    "            place += 1\n",
    "    \n",
    "    while len(feature_vector) < 260:\n",
    "        feature_vector.append(0)\n",
    "    return feature_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test nameToVec\n",
    "print(nameToVec(\"nick fausti\"))\n",
    "nameToVec(\"nick fausti\") == [0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,\n",
    "     0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,\n",
    "     0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,\n",
    "     0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,\n",
    "     0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,\n",
    "     0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,\n",
    "     1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,\n",
    "     0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,\n",
    "     0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,\n",
    "     0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "names: 700\n",
      "labels: 700\n",
      "X: 700\n"
     ]
    }
   ],
   "source": [
    "# Feature Extraction\n",
    "from sklearn import tree\n",
    "# X should be a list of n=260 vectors, 26 * 10 characters (5 in first, 5 in lastname)\n",
    "# X = [[0, 0], [1, 1]]\n",
    "names = []\n",
    "labels = []\n",
    "\n",
    "X = []\n",
    "\n",
    "with open(\"hw1/badges/badges.modified.data.train\", \"r\") as f:\n",
    "    for line in f:\n",
    "        data = line.split(\" \")\n",
    "        names.append(data[1] + \" \" + data[2].strip())\n",
    "        if data[0] == \"+\":\n",
    "            labels.append(1)\n",
    "        else:\n",
    "            labels.append(0)\n",
    "\n",
    "for name in names:\n",
    "    X.append(nameToVec(name))\n",
    "\n",
    "\n",
    "print(\"names: \" + str(len(names)))\n",
    "print(\"labels: \" + str(len(labels)))\n",
    "print(\"X: \" + str(len(X)))\n",
    "\n",
    "\n",
    "# y is a boolean label -/+ ie. 0/1\n",
    "# y = [0, 1]\n",
    "\n",
    "# clf = tree.DecisionTreeClassifier()\n",
    "# clf = clf.fit(X,y)\n",
    "# clf.predict([[2.,2.]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nick/cis519/venv/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "clf = SGDClassifier(loss=\"log\")\n",
    "clf.fit(np.array(X),labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n",
      "[0]\n",
      "[1]\n"
     ]
    }
   ],
   "source": [
    "print(clf.predict([nameToVec(\"shai bendavid\")]))\n",
    "print(clf.predict([nameToVec(\"nick fausti\")]))\n",
    "print(clf.predict([nameToVec(\"david mathias\")]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each algorithm you experiment with, (i) run five-fold cross validation on the given\n",
    "# data set.\n",
    "#\n",
    "# pA is the average accuracy over the five folds\n",
    "#\n",
    "# In addition\n",
    "# to pA, record also the performance of your algorithm on the training data, trA. This\n",
    "# will also be the average you get over the five training folds. (ii) Calculate the 99%\n",
    "# confidence interval of this estimate using Student’s t-test.\n",
    "\n",
    "# SGD\n",
    "\n",
    "def makeFoldsFromFile(foldNum):\n",
    "    X = []\n",
    "    names = []\n",
    "    labels = []\n",
    "    \n",
    "    with open(\"hw1/badges/badges.modified.data.fold\" + str(foldNum), \"r\") as f:\n",
    "        for line in f:\n",
    "            data = line.split(\" \")\n",
    "            names.append(data[1] + \" \" + data[2].strip())\n",
    "            if data[0] == \"+\":\n",
    "                labels.append(1)\n",
    "            else:\n",
    "                labels.append(0)\n",
    "    \n",
    "    for name in names:\n",
    "        X.append(nameToVec(name))\n",
    "    return (X, names, labels)\n",
    "\n",
    "# vectors, strings_of_names, labels\n",
    "X1, test1, y1 = makeFoldsFromFile(1)\n",
    "X2, test2, y2 = makeFoldsFromFile(2)\n",
    "X3, test3, y3 = makeFoldsFromFile(3)\n",
    "X4, test4, y4 = makeFoldsFromFile(4)\n",
    "X5, test5, y5 = makeFoldsFromFile(5)\n",
    "\n",
    "# FOLD GROUP 1\n",
    "fold1Features = X1 + X2 + X3 + X4\n",
    "fold1Labels = y1 + y2 + y3 + y4\n",
    "fold1test = test5\n",
    "\n",
    "# FOLD GROUP 2\n",
    "fold2Features = X2 + X3 + X4 + X5\n",
    "fold2Labels = y2 + y3 + y4 + y5\n",
    "fold2test = test1\n",
    "\n",
    "# FOLD GROUP 3\n",
    "fold3Features = X3 + X4 + X5 + X1\n",
    "fold3Labels = y3 + y4 + y5 + y1\n",
    "fold3test = test2\n",
    "\n",
    "# FOLD GROUP 4\n",
    "fold4Features = X4 + X5 + X1 + X2\n",
    "fold4Labels = y4 + y5 + y1 + y2\n",
    "fold4test = test3\n",
    "\n",
    "# FOLD GROUP 5\n",
    "fold5Features = X5 + X1 + X2 + X3\n",
    "fold5Labels = y5 + y1 +y2 + y3\n",
    "fold5test = test4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6657142857142857\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nick/cis519/venv/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "clf1 = clf.fit(fold1Features, fold1Labels)\n",
    "score1 = clf.score(X5,y5)\n",
    "\n",
    "clf2 = clf.fit(fold2Features, fold2Labels)\n",
    "score2 = clf2.score(X1,y1) \n",
    "\n",
    "clf3 = clf.fit(fold3Features, fold3Labels)\n",
    "score3 = clf3.score(X2,y2)\n",
    "\n",
    "clf4 = clf.fit(fold4Features, fold4Labels)\n",
    "score4 = clf4.score(X3,y3)\n",
    "\n",
    "clf5 = clf.fit(fold5Features, fold5Labels)\n",
    "score5 = clf5.score(X4,y4) \n",
    "\n",
    "print(np.average([score1,score2,score3,score4,score5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6642857142857143\n"
     ]
    }
   ],
   "source": [
    "# print( sklearn.metrics.accuracy_score(predictions, y5) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'drago indjic'"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# repeat what I did above, swapping out the 5th test labels for\n",
    "# others\n",
    "# average that score, repeat with decision trees, stumps, and using\n",
    "# stumps as features for SGD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fold_features_list = [fold1Features,fold2Features,\n",
    "                 fold3Features,fold4Features,fold5Features]\n",
    "\n",
    "fold_labels_list = [fold1Labels,fold2Labels,\n",
    "                 fold3Labels,fold4Labels,fold5Labels]\n",
    "\n",
    "fold_X = [X5, X1, X2, X3, X4]\n",
    "fold_y = [y5, y1, y2, y3, y4]\n",
    "\n",
    "dtMaxScores = []\n",
    "\n",
    "dtMaxClf = sklearn.tree.DecisionTreeClassifier()\n",
    "\n",
    "for i, feat in enumerate(fold_features_list):\n",
    "    dtMaxClf.fit(feat, fold_labels_list[i])\n",
    "    dtMaxScores.append(dtMaxClf.score(fold_X[i], fold_y[i]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7185714285714285\n"
     ]
    }
   ],
   "source": [
    "# No-max decision tree\n",
    "print(np.average(dtMaxScores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6328571428571428\n"
     ]
    }
   ],
   "source": [
    "dt4Scores = []\n",
    "\n",
    "dt4clf = sklearn.tree.DecisionTreeClassifier(max_depth=4)\n",
    "\n",
    "for i, feat in enumerate(fold_features_list):\n",
    "    dt4clf.fit(feat, fold_labels_list[i])\n",
    "    dt4Scores.append(dt4clf.score(fold_X[i], fold_y[i]))\n",
    "\n",
    "print(np.average(dt4Scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7171428571428572\n"
     ]
    }
   ],
   "source": [
    "# Decision tree with max-depth=8\n",
    "dt8Scores = []\n",
    "\n",
    "dt8clf = sklearn.tree.DecisionTreeClassifier(max_depth=8)\n",
    "\n",
    "for i, feat in enumerate(fold_features_list):\n",
    "    dt8clf.fit(feat, fold_labels_list[i])\n",
    "    dt8Scores.append(dt8clf.score(fold_X[i], fold_y[i]))\n",
    "\n",
    "print(np.average(dt8Scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Stumps as features\n",
    "boostNamesVecsLabels = [] #[(name, feature_vec, label)]\n",
    "boostLabels = [] \n",
    "\n",
    "boostX = []\n",
    "\n",
    "with open(\"hw1/badges/badges.modified.data.train\", \"r\") as f:\n",
    "    for line in f:\n",
    "        name = \"\"\n",
    "        label = 0\n",
    "        \n",
    "        data = line.split(\" \")\n",
    "        name = data[1] + \" \" + data[2].strip()\n",
    "        \n",
    "        feature_vec = nameToVec(name)\n",
    "        \n",
    "        if data[0] == \"+\":\n",
    "            label = 1\n",
    "        else:\n",
    "            label = 0\n",
    "        results = [name, feature_vec, label]\n",
    "        boostNamesVecsLabels.append(results)\n",
    "\n",
    "# for name in boostNames:\n",
    "#     boostX.append(nameToVec(name))\n",
    "\n",
    "# should be a vector of len 100 (100 predictions from\n",
    "# decisionStubs trained on 50% of data)\n",
    "\n",
    "# list of DT objects\n",
    "classifiers = []\n",
    "wordPredictions = []\n",
    "\n",
    "for i in range(100):\n",
    "    # take 700/2 names to use as 50% train sample\n",
    "    labelDict = {}\n",
    "    trainNames = []\n",
    "    trainVecs = []\n",
    "    \n",
    "    for i,v in enumerate(boostNamesVecsLabels):\n",
    "        # set name : label\n",
    "        labelDict[v[0]] = v[2]\n",
    "        trainNames.append(v[0])\n",
    "    \n",
    "    trainSubset = np.random.choice(trainNames, 350, replace=False)\n",
    "    \n",
    "    trainX = []\n",
    "    trainY = []\n",
    "    \n",
    "    for name in trainSubset:\n",
    "        trainX.append(nameToVec(name))\n",
    "        trainY.append(labelDict[name])\n",
    "\n",
    "    clf = sklearn.tree.DecisionTreeClassifier(max_depth=8)\n",
    "    clf.fit(trainX, trainY)\n",
    "    \n",
    "    classifiers.append(clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "stumps = []\n",
    "stumpVec = []\n",
    "\n",
    "# for name in data.fold1 + data.fold2 + data.fold3 + data.fold4 + data.fold5\n",
    "for clf in classifiers:\n",
    "#     stumpVector.append(clf.predict([nameToVec(\"nick fausti\")])[0])\n",
    "    stumpVec.append(clf.predict([nameToVec(\"nick fausti\")])[0])\n",
    "\n",
    "stumps.append(stumpVec)\n",
    "\n",
    "# Now do 5-fold cross validation with new model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "560"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fold_features_list = [fold1Features,fold2Features,\n",
    "#                  fold3Features,fold4Features,fold5Features]\n",
    "\n",
    "# fold_labels_list = [fold1Labels,fold2Labels,\n",
    "#                  fold3Labels,fold4Labels,fold5Labels]\n",
    "\n",
    "# fold_X = [X5, X1, X2, X3, X4]\n",
    "# fold_y = [y5, y1, y2, y3, y4]\n",
    "\n",
    "# stumpScores = []\n",
    "\n",
    "# dtMaxClf = sklearn.tree.DecisionTreeClassifier()\n",
    "\n",
    "# for i, feat in enumerate(fold_features_list):\n",
    "#     dtMaxClf.fit(feat, fold_labels_list[i])\n",
    "#     dtMaxScores.append(dtMaxClf.score(fold_X[i], fold_y[i]))\n",
    "len(fold1Labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeStumpFoldsFromFile(foldNum):\n",
    "    X = []\n",
    "    names = []\n",
    "    labels = []\n",
    "    stumps = []\n",
    "    \n",
    "    with open(\"hw1/badges/badges.modified.data.fold\" + str(foldNum), \"r\") as f:\n",
    "        for line in f:\n",
    "            data = line.split(\" \")\n",
    "            names.append(data[1] + \" \" + data[2].strip())\n",
    "            if data[0] == \"+\":\n",
    "                labels.append(1)\n",
    "            else:\n",
    "                labels.append(0)\n",
    "    \n",
    "    for name in names:\n",
    "        stumpVec = []\n",
    "        X.append(nameToVec(name))\n",
    "        # append prediction from 100 different classifiers to stumpVec\n",
    "        for clf in classifiers:\n",
    "        #     stumpVector.append(clf.predict([nameToVec(\"nick fausti\")])[0])\n",
    "            stumpVec.append(clf.predict([nameToVec(name)])[0])\n",
    "        stumps.append(stumpVec)\n",
    "\n",
    "    return (stumps, X, names, labels)\n",
    "\n",
    "\n",
    "stumpVec = []\n",
    "\n",
    "# for name in data.fold1 + data.fold2 + data.fold3 + data.fold4 + data.fold5\n",
    "\n",
    "\n",
    "stumps.append(stumpVec)\n",
    "\n",
    "# vectors, strings_of_names, labels\n",
    "stump1, X1, test1, y1 = makeStumpFoldsFromFile(1)\n",
    "stump2, X2, test2, y2 = makeStumpFoldsFromFile(2)\n",
    "stump3, X3, test3, y3 = makeStumpFoldsFromFile(3)\n",
    "stump4, X4, test4, y4 = makeStumpFoldsFromFile(4)\n",
    "stump5, X5, test5, y5 = makeStumpFoldsFromFile(5)\n",
    "\n",
    "# FOLD GROUP 1\n",
    "fold1StumpFeatures = stump1 + stump2 + stump3 + stump4\n",
    "fold1Features = X1 + X2 + X3 + X4\n",
    "fold1Labels = y1 + y2 + y3 + y4\n",
    "fold1test = test5\n",
    "fold1StumpTest = stump5\n",
    "\n",
    "# # FOLD GROUP 2\n",
    "fold2StumpFeatures = stump2 + stump3 + stump4 + stump5\n",
    "fold2Features = X2 + X3 + X4 + X5\n",
    "fold2Labels = y2 + y3 + y4 + y5\n",
    "fold2test = test1\n",
    "fold2StumpTest = stump1\n",
    "\n",
    "# FOLD GROUP 3\n",
    "fold3StumpFeatures = stump3 + stump4 + stump5 + stump1\n",
    "fold3Features = X3 + X4 + X5 + X1\n",
    "fold3Labels = y3 + y4 + y5 + y1\n",
    "fold3test = test2\n",
    "fold3StumpTest = stump2\n",
    "\n",
    "# FOLD GROUP 4\n",
    "fold4StumpFeatures = stump4 + stump5 + stump1 + stump2\n",
    "fold4Features = X4 + X5 + X1 + X2\n",
    "fold4Labels = y4 + y5 + y1 + y2\n",
    "fold4test = test3\n",
    "fold4StumpTest = stump3\n",
    "\n",
    "# FOLD GROUP 5\n",
    "fold5StumpFeatures = stump5 + stump1 + stump2 + stump3\n",
    "fold5Features = X5 + X1 + X2 + X3\n",
    "fold5Labels = y5 + y1 +y2 + y3\n",
    "fold5test = test4\n",
    "fold5StumpTest = stump4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testcase = []\n",
    "for clf in classifiers:\n",
    "    testcase.append(clf.predict([nameToVec(\"neta rivkin\")])[0])\n",
    "stump1[0] == testcase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nick/cis519/venv/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n",
       "       eta0=0.0, fit_intercept=True, l1_ratio=0.15,\n",
       "       learning_rate='optimal', loss='log', max_iter=None, n_iter=None,\n",
       "       n_jobs=1, penalty='l2', power_t=0.5, random_state=None,\n",
       "       shuffle=True, tol=None, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = SGDClassifier(loss=\"log\")\n",
    "clf.fit(fold1StumpFeatures, fold1Labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(stump5, y5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nick/cis519/venv/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "stump_cv_features_list = [fold1StumpFeatures,fold2StumpFeatures,\n",
    "                 fold3StumpFeatures,fold4StumpFeatures,fold5StumpFeatures]\n",
    "\n",
    "stump_cv_labels_list = [fold1Labels,fold2Labels,\n",
    "                  fold3Labels,fold4Labels,fold5Labels]\n",
    "\n",
    "stump_fold_X = [stump5, stump1, stump2, stump3, stump4]\n",
    "stump_fold_y = [y5, y1, y2, y3, y4]\n",
    "\n",
    "stumpScores = []\n",
    "\n",
    "stumpClf = SGDClassifier(loss=\"log\")\n",
    "\n",
    "for i, feat in enumerate(stump_cv_features_list):\n",
    "    stumpClf.fit(feat, stump_cv_labels_list[i])\n",
    "    stumpScores.append(stumpClf.score(stump_fold_X[i], stump_fold_y[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8271428571428572"
      ]
     },
     "execution_count": 309,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.average(stumpScores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
